{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxiiZdqZ/dgURjSgHn4hwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanda74775/MMO_LAB/blob/main/MMO_LAB7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNRWquxme3C-",
        "outputId": "81be7045-de3b-42f2-8841-79671bc94701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym numpy matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "R2oxQs0igC1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определение среды\n",
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "_jbQTTQSfsDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Дискретизация пространства состояний\n",
        "n_bins = (6, 12, 6, 12)  # количество бинов для каждого измерения\n",
        "lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -np.radians(50)]\n",
        "upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], np.radians(50)]\n"
      ],
      "metadata": {
        "id": "MOCFCNqVgPip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discretize_state(state, n_bins, lower_bounds, upper_bounds):\n",
        "    ratios = [(state[i] - lower_bounds[i]) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(state))]\n",
        "    new_state = [int(round((n_bins[i] - 1) * ratios[i])) for i in range(len(state))]\n",
        "    new_state = [min(n_bins[i] - 1, max(0, new_state[i])) for i in range(len(state))]\n",
        "    return tuple(new_state)"
      ],
      "metadata": {
        "id": "VZdNl1OfgSD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa(env, episodes, alpha, gamma, epsilon, n_bins, lower_bounds, upper_bounds):\n",
        "    q_table = np.zeros(n_bins + (env.action_space.n,))\n",
        "\n",
        "    def choose_action(state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(q_table[state])\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = discretize_state(env.reset(), n_bins, lower_bounds, upper_bounds)\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            next_state_raw, reward, done, _ = env.step(action)\n",
        "            next_state = discretize_state(next_state_raw, n_bins, lower_bounds, upper_bounds)\n",
        "            next_action = choose_action(next_state, epsilon)\n",
        "\n",
        "            q_table[state + (action,)] += alpha * (reward + gamma * q_table[next_state + (next_action,)] - q_table[state + (action,)])\n",
        "\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "QgJbCGMBgUOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(env, episodes, alpha, gamma, epsilon, n_bins, lower_bounds, upper_bounds):\n",
        "    q_table = np.zeros(n_bins + (env.action_space.n,))\n",
        "\n",
        "    def choose_action(state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(q_table[state])\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = discretize_state(env.reset(), n_bins, lower_bounds, upper_bounds)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = choose_action(state, epsilon)\n",
        "            next_state_raw, reward, done, _ = env.step(action)\n",
        "            next_state = discretize_state(next_state_raw, n_bins, lower_bounds, upper_bounds)\n",
        "\n",
        "            q_table[state + (action,)] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state + (action,)])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return q_table"
      ],
      "metadata": {
        "id": "cZmI9rYpgWxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def double_q_learning(env, episodes, alpha, gamma, epsilon, n_bins, lower_bounds, upper_bounds):\n",
        "    q_table1 = np.zeros(n_bins + (env.action_space.n,))\n",
        "    q_table2 = np.zeros(n_bins + (env.action_space.n,))\n",
        "\n",
        "    def choose_action(state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(q_table1[state] + q_table2[state])\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = discretize_state(env.reset(), n_bins, lower_bounds, upper_bounds)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = choose_action(state, epsilon)\n",
        "            next_state_raw, reward, done, _ = env.step(action)\n",
        "            next_state = discretize_state(next_state_raw, n_bins, lower_bounds, upper_bounds)\n",
        "\n",
        "            if np.random.random() < 0.5:\n",
        "                best_next_action = np.argmax(q_table1[next_state])\n",
        "                q_table1[state + (action,)] += alpha * (reward + gamma * q_table2[next_state + (best_next_action,)] - q_table1[state + (action,)])\n",
        "            else:\n",
        "                best_next_action = np.argmax(q_table2[next_state])\n",
        "                q_table2[state + (action,)] += alpha * (reward + gamma * q_table1[next_state + (best_next_action,)] - q_table2[state + (action,)])\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "    return q_table1 + q_table2"
      ],
      "metadata": {
        "id": "vN2_BJDjgfup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры обучения\n",
        "episodes = 1000\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1"
      ],
      "metadata": {
        "id": "Afkslfq-gqHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение\n",
        "q_table_sarsa = sarsa(env, episodes, alpha, gamma, epsilon, n_bins, lower_bounds, upper_bounds)\n",
        "q_table_q_learning = q_learning(env, episodes, alpha, gamma, epsilon, n_bins, lower_bounds, upper_bounds)\n",
        "q_table_double_q = double_q_learning(env, episodes, alpha, gamma, epsilon, n_bins, lower_bounds, upper_bounds)\n"
      ],
      "metadata": {
        "id": "Fyo0SR6Rgs5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Оценка политики\n",
        "def evaluate_policy(env, q_table, episodes=100):\n",
        "    total_rewards = []\n",
        "    for _ in range(episodes):\n",
        "        state = discretize_state(env.reset(), n_bins, lower_bounds, upper_bounds)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = np.argmax(q_table[state])\n",
        "            next_state_raw, reward, done, _ = env.step(action)\n",
        "            next_state = discretize_state(next_state_raw, n_bins, lower_bounds, upper_bounds)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "        total_rewards.append(total_reward)\n",
        "    return np.mean(total_rewards)\n"
      ],
      "metadata": {
        "id": "e02HcrajgvmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward_sarsa = evaluate_policy(env, q_table_sarsa)\n",
        "mean_reward_q_learning = evaluate_policy(env, q_table_q_learning)\n",
        "mean_reward_double_q = evaluate_policy(env, q_table_double_q)\n"
      ],
      "metadata": {
        "id": "jjJuwqYkgy25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"SARSA mean reward: {mean_reward_sarsa}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x9EQD9dg1BR",
        "outputId": "a938f521-dd55-4409-9f9a-34ce8b7e3005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SARSA mean reward: 9.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Q-Learning mean reward: {mean_reward_q_learning}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqMir90lg3VB",
        "outputId": "935b2d0b-b6cd-415a-9115-a048c945d84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Learning mean reward: 10.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Double Q-Learning mean reward: {mean_reward_double_q}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G5KdDv9g5qZ",
        "outputId": "c71b2c68-a4d9-4323-a281-6000b77b480c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Double Q-Learning mean reward: 10.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pp1VOtJsg8fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}